---
jupyter:
  jupytext:
    cell_metadata_filter: -all
    formats: ipynb,md
    main_language: python
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.17.3
---


```python
import torch
import torch.nn as nn

```

使用pyTorch内置的损失函数和优化器来计算loss和更新参数
loss = nn.MSELoss()

optimizer = torch.optim.SGD([w],lr = lr)
...
optimizer.step()
optimizer.zero_grad()

```python
# f = w*x
X = torch.tensor([1,2,3,4], dtype=torch.float32)
Y = torch.tensor([2,4,6,8], dtype=torch.float32)
w = torch.tensor(0.0,  dtype=torch.float32, requires_grad = True)
#model prediction
def forward(x):
	return w*x

# loss 
loss = nn.MSELoss()


# gradient 
# MSE = 1/N*(w*x-y)**2	
# dloss/dw = 1/N*2x*(w*x-y)
# def gradient(x,y,y_pred):
# return np.dot(2*x, y_pred-y).mean()
 
print(f'训练前的预测值：f(5) = {forward(5):.3f}')

#Training
lr = 0.01
n_iters = 100

optimizer = torch.optim.SGD([w],lr = lr)

for epoch in range(n_iters):

	y_pred = forward(X)
	l=loss(Y,y_pred)
	# gradient = backward pass
	l.backward()

	optimizer.step()

	optimizer.zero_grad() #需要重置.grad
	
	if epoch % 10 == 0:
		print(f'epoch {epoch+1} w = {w:.3f}, loss = {l:.8f}')
		
print(f'训练后的预测值：f(5) = {forward(5):.3f}')
```

使用pytorch内置的线性模型
```python
# f = w*x
X = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)
Y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)

X_test = torch.tensor([5], dtype=torch.float32)

n_samples,n_features = X.shape
print(n_samples,n_features)

input_size = n_features
output_size = n_features
model = nn.Linear(input_size, output_size)



print(f'训练前的预测值：f(5) = {model(X_test).item():.3f}')

#Training
lr = 0.01
n_iters = 200
loss = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(),lr = lr)

for epoch in range(n_iters):
	y_pred = model(X)
	l=loss(Y,y_pred)
	
	# gradient = backward pass
	l.backward()

	optimizer.step()

	optimizer.zero_grad() #需要重置.grad
	
	if epoch % 10 == 0:
		[w,b] = model.parameters()
		print(f'epoch {epoch+1} w = {w[0][0].item():.3f}, loss = {l:.8f}')
		
print(f'训练后的预测值：f(5) = {model(X_test).item():.3f}')
```

自定义一个模型LinearRegression
```python
# f = w*x
X = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)
Y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)

X_test = torch.tensor([5], dtype=torch.float32)

n_samples,n_features = X.shape
print(n_samples,n_features)

input_size = n_features
output_size = n_features
model = nn.Linear(input_size, output_size)

class LinearRegression(nn.Module):
	def __init__(self, input_dim, output_dim):
		super(LinearRegression, self).__init__()
		#define layers
		self.lin = nn.Linear(input_dim, output_dim)
	def forward(self, x):
		return self.lin(x)

model = LinearRegression(input_size, output_size)
print(f'训练前的预测值：f(5) = {model(X_test).item():.3f}')

#Training
lr = 0.01
n_iters = 200
loss = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(),lr = lr)

for epoch in range(n_iters):
	y_pred = model(X)
	l=loss(Y,y_pred)
	
	# gradient = backward pass
	l.backward()

	optimizer.step()

	optimizer.zero_grad() #需要重置.grad
	
	if epoch % 10 == 0:
		[w,b] = model.parameters()
		print(f'epoch {epoch+1} w = {w[0][0].item():.3f}, loss = {l:.8f}')
		
print(f'训练后的预测值：f(5) = {model(X_test).item():.3f}')
```
