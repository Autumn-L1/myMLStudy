---
jupyter:
  jupytext:
    cell_metadata_filter: -all
    formats: ipynb,md
    main_language: python
    text_representation:
      extension: .md
      format_name: markdown
      format_version: "1.3"
      jupytext_version: 1.17.3
---


```python
import torch
import numpy as np
```

```python
# f = w*x
X = np.array([1,2,3,4], dtype=np.float32)
Y = np.array([2,4,6,8], dtype=np.float32)
w = 0.0
#model prediction
def forward(x):
	return w*x

# loss = MSE
def loss(y, y_pred):
	return ((y_pred-y)**2).mean()

# gradient 
# MSE = 1/N*(w*x-y)**2	
# dloss/dw = 1/N*2x*(w*x-y)
def gradient(x,y,y_pred):
 return np.dot(2*x, y_pred-y).mean()
 
print(f'训练前的预测值：f(5) = {forward(5):.3f}')

#Training
lr = 0.01
n_iters = 20

for epoch in range(n_iters):
	y_pred = forward(X)
	l=loss(Y,y_pred)
	dw=gradient(X,Y,y_pred)
	w-=lr*dw
	if epoch % 2 == 0:
		print(f'epoch {epoch+1} w = {w:.3f}, loss = {l:.8f}')
		
print(f'训练后的预测值：f(5) = {forward(5):.3f}')
```

```python
# f = w*x
X = torch.tensor([1,2,3,4], dtype=torch.float32)
Y = torch.tensor([2,4,6,8], dtype=torch.float32)
w = torch.tensor(0.0,  dtype=torch.float32, requires_grad = True)
#model prediction
def forward(x):
	return w*x

# loss = MSE
def loss(y, y_pred):
	return ((y_pred-y)**2).mean()

# gradient 
# MSE = 1/N*(w*x-y)**2	
# dloss/dw = 1/N*2x*(w*x-y)
# def gradient(x,y,y_pred):
# return np.dot(2*x, y_pred-y).mean()
 
print(f'训练前的预测值：f(5) = {forward(5):.3f}')

#Training
lr = 0.01
n_iters = 100

for epoch in range(n_iters):
	y_pred = forward(X)
	l=loss(Y,y_pred)
	
	# gradient = backward pass
	l.backward()
	with torch.no_grad(): #w的更新无需被计算梯度
		w-=lr*w.grad
	w.grad.zero_() #需要重置.grad
	
	if epoch % 10 == 0:
		print(f'epoch {epoch+1} w = {w:.3f}, loss = {l:.8f}')
		
print(f'训练后的预测值：f(5) = {forward(5):.3f}')
```

注意收敛的速度并不相同，因为.backward计算的方式并不是单纯的一阶导数方程计算