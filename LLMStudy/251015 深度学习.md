什么是深度学习：通过 包含隐藏层的神经网络 直接从数据中学习特征和任务的 机器学习技术
为什么使用深度学习： 可以直接从原始数据中学习
深度学习得到推广的基础：大数据，硬件发展，软件架构发展

神经网络、输入层、隐藏层、输出层

前向传播：前神经元乘以权重影响下一神经元
	权重 Weight： 神经元的重要性
	偏置 Bias：补偿输入分布中心的偏移（非零）
反向传播：预测错误时，使用损失函数，调整权重和偏置，使其更适合预测模型

神经网络常用术语
激活函数 Activation Function：将神经元的输入映射到输出端的函数
	![[v2-91e1b17ef9b61256739749feff3cea10_1440w.jpg]]
	常用激活函数
		Sigmoid：归一化、梯度平滑、可微、明确预测 / 梯度消失、非零中心化、指数运算计算成本高
		Tanh：Sigmond优势+零中心化 / 梯度消失、指数运算计算成本高
		ReLU (Rectified Linear Unit)：改善梯度消失、计算快 / 可能导致永远不被激活的神经元-Dead ReLU问题、非零中心化
		Leaky ReLU： ReLU优势+缓解Dead ReLU

损失函数 Loss Function：衡量模型的输出与真实值之间的差距

优化器 Optimizer: 在反向传播过程中，调整神经网络参数，以最小化损失函数的算法
	最流行的方案类型：梯度下降 Gradient Descendent
		全梯度下降 BGD: 每次使用全部样本计算梯度
		随机梯度下降 SGD(stochastic gradient descent): 每次使用一个/一批样本计算梯度
			高效、在线学习、部分克服局部最优 / 不稳定性、 局部最优
		动量法 Momentum: 当前时刻的梯度是从开始时刻到当前时刻的梯度指数加权平均
			下降速度快、减少振荡 / 
		自适应梯度法 AdaGrad(**Ada**ptive **grad**ient Estimation)：记录每次迭代过程中的前进方向和距离，具有较大偏导的参数有快速下降的学习率
			学习率自适应调整、凸问题时快速收敛 / 迭代后期收敛缓慢
		Adadelta/RMSProp :使用指数加权移动平均的方法计算累积梯度，以丢弃遥远的梯度历史信息
			AdaGrad+非凸条件下效果更好
		Adam(**Ada**ptive **M**oment Estimation): Momentum+RMSProp+偏差修正
			超参数鲁棒性、适合大规模数据 /

参数和超参数

周期 Epoch: 完整遍历一次数据集的训练过程
批次 Batch: 一次性输入的一组样本
步骤 Step:  一次参数更新。通常完成一个batch的训练就是完成了一个Step
迭代 Iteration: 一次完整的网络训练（一次前向传播和反向传播），通常和Step等价。


监督学习
无监督学习
强化学习

过拟合解决
暂退法 Dropout：每次训练忽略一些神经元
数据增强  Data Augmentation：对真实数据施加各种变化生成新的样本，在物体识别中特别有效
早停法 Early Stopping： 在验证误差开始上升时停止训练

常见神经网络
	全连接前馈网络
	RNN
		可以处理序列数据
		仅短期记忆。由于梯度消失问题，无法学习长距离的依赖关系
		两种变体引入门（gate）解决长期问题：LSTM，GRNN
	CNN
		卷积 Convolve
		池化 Pool